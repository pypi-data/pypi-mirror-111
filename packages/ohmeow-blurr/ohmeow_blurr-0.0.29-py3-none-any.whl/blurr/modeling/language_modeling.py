# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_modeling-language-modeling.ipynb (unless otherwise specified).

__all__ = ['lm_accuracy', 'BlearnerForLM']

# Cell
import ast

import torch
from transformers import *
from fastai.text.all import *
from sklearn.metrics import accuracy_score

from ..utils import *
from ..data.core import *
from ..data.language_modeling import *
from .core import *

# Cell
def lm_accuracy(preds, targs):
        preds = preds.argmax(dim=-1)

        msk = torch.where(targs != -100, 1, 0).bool()
        preds = torch.masked_select(preds, msk)
        targs = torch.masked_select(targs, msk)

        return accuracy_score(targs.cpu(), preds.cpu())

# Cell
@typedispatch
def show_results(x:HF_CausalLMInput, y, samples, outs, learner, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer and ignore token to decode
    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    ignore_token_id = hf_before_batch_tfm.ignore_token_id

    res = L([(
        hf_tokenizer.decode(s[0], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(pred[0], skip_special_tokens=True)[:trunc_at],
    ) for s, pred in zip(samples, outs) ])

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@typedispatch
def show_results(x:HF_MLMInput, y, samples, outs, learner, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer and ignore token to decode
    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    ignore_token_id = hf_before_batch_tfm.ignore_token_id

    # grab our mask token id and do-not-mask token ids
    mask_token_id = hf_tokenizer.mask_token_id
    dnm_tok_ids = hf_before_batch_tfm.lm_strategy.dnm_tok_ids

    res = L()
    for s, t in zip(samples, outs):
        # exclue dnm tokens from input
        inps = [ hf_tokenizer.decode(tok_id)
                if (tok_id == mask_token_id or s[1][idx] == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[0]) if (tok_id not in dnm_tok_ids) ]

        # replaced masked tokens with "[{actual_token}]"
        trgs = [ hf_tokenizer.decode(s[0][idx])
                if (tok_id == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        # same as above except we replace the [MASK] with the PREDICTED token
        preds = [ hf_tokenizer.decode(s[0][idx])
                 if (tok_id == ignore_token_id)
                 else f'[{hf_tokenizer.decode(t[0][idx])}]'
                 for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        res.append((' '.join(inps[:trunc_at]).strip(),
                    ' '.join(trgs[:trunc_at]).strip(),
                    ' '.join(preds[:trunc_at]).strip()))

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@patch
def blurr_fill_mask(self:Learner, inp, n_preds=1, **kwargs):
    """TODO"""
    # grab the Hugging Face tokenizer from the learner's dls.tfms
    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    tok_kwargs = hf_before_batch_tfm.tok_kwargs

    # grab the text generation kwargs
    text_gen_kwargs = hf_before_batch_tfm.text_gen_kwargs if (len(kwargs) == 0) else kwargs

    if (isinstance(inp, str)):
        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)
    else:
        # note (10/30/2020): as of pytorch 1.7, this has to be a plain ol tensor (not a subclass of TensorBase)
        input_ids = inp.as_subclass(Tensor)

    input_ids = input_ids.to(self.model.hf_model.device)
    mask_token_index = torch.where(input_ids == hf_tokenizer.mask_token_id)[1]

    outputs = self.model.hf_model(input_ids)
    mask_token_logits = outputs.logits[0, mask_token_index, :]
    preds = torch.topk(mask_token_logits, n_preds, dim=-1).indices[0].tolist()

    outputs = [ inp.replace(hf_tokenizer.mask_token, hf_tokenizer.decode([tok_id]).strip())
               for tok_id in preds ]

    return outputs

# Cell
@delegates(Blearner.__init__)
class BlearnerForLM(Blearner):

    def __init__(self, dls, hf_model, **kwargs):
        kwargs['loss_func'] = HF_PreCalculatedLoss()
        super().__init__(dls, hf_model, **kwargs)

    @classmethod
    def get_model_cls(self, lm_type):
        return AutoModelForCausalLM if (lm_type == LMType.CAUSAL) else AutoModelForMaskedLM

    @classmethod
    def _create_learner(cls, data, pretrained_model_name_or_path, preprocess_func,
                        lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs):

        lm_type = lm_strategy_cls.get_lm_type()

        # get our hf objects
        model_cls = cls.get_model_cls(lm_type=lm_type)
        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path,
                                                                          model_cls=model_cls)

        # if we need to preprocess the raw data before creating our DataLoaders
        if (preprocess_func):
            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, lm_type, lm_strategy_cls)

        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here
        if (hf_tokenizer.pad_token is None):
            hf_tokenizer.add_special_tokens({'pad_token': '<pad>'})
            hf_config.pad_token_id = hf_tokenizer.get_vocab()['<pad>']
            hf_model.resize_token_embeddings(len(hf_tokenizer))

        # build dblock and dls
        if (isinstance(data, pd.DataFrame)):
            get_x = ColReader(text_attr)
        else:
            get_x = ItemGetter(text_attr)

        bbtfm = HF_LMBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=lm_strategy_cls)

        input_return_type = HF_CausalLMInput if (lm_type == LMType.CAUSAL) else HF_MLMInput
        blocks = (HF_TextBlock(before_batch_tfm=bbtfm, input_return_type=input_return_type), noop)

        dblock = DataBlock(blocks=blocks, get_x=get_x, splitter=dblock_splitter)
        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance with default metrics (optional)
        learner_kwargs['metrics'] = learner_kwargs.pop('metrics', [lm_accuracy, perplexity])
        return cls(dls, hf_model, **learner_kwargs.copy())

    @classmethod
    def from_dataframe(cls, df, pretrained_model_name_or_path, preprocess_func=None,
                       lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=ColSplitter(),
                       dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(df, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs)


    @classmethod
    def from_csv(cls, csv_file, pretrained_model_name_or_path, preprocess_func=None,
                 lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=ColSplitter(),
                 dl_kwargs={}, learner_kwargs={}):

        df = pd.read_csv(csv_file)
        return cls.from_dataframe(df, pretrained_model_name_or_path, preprocess_func,
                                  lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs)

    @classmethod
    def from_dictionaries(cls, ds, pretrained_model_name_or_path, preprocess_func=None,
                          lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=RandomSplitter(),
                          dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(ds, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs)